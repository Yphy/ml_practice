{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_RNN models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yphy/ml_practice/blob/master/11_RNN_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV40U7mLZkN4",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow 실습 : Recurrent Neural Networks (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5X9HqjFaFVK",
        "colab_type": "text"
      },
      "source": [
        "## RNN layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAgXBP8LtArB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6madZY2m1zsO",
        "colab_type": "text"
      },
      "source": [
        "- Tensorflow에서 제공하는 일반적인 RNN layers\n",
        "  - 기본 RNN: tf.keras.layers.SimpleRNN\n",
        "  - LSTM: tf.keras.layers.LSTM\n",
        "  - GRU: tf.keras.layers.GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR32V84D2MQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "f2fe0030-5d60-4fdf-a74a-ad9ebe84ba0e"
      },
      "source": [
        "# 기본적으로 RNN layer의 입력 tensor shape는 [batch, timesteps, feature]로 3D tensor\n",
        "# 여기서 input은 16개의 time step을 가지고, 각 time step 마다 32차원의 값을 가지는 형태\n",
        "sequence_len = 16\n",
        "feature_dim = 32\n",
        "input_sequence = tf.keras.Input(shape=(sequence_len, feature_dim))\n",
        "\n",
        "# units은 RNN layer의 hidden size를 의미함\n",
        "units = 64\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# input sequence를 입력해서 모델을 build\n",
        "rnn(input_sequence)\n",
        "# RNN layer의 출력 shape를 보면, 시간 축이 사라진 것을 확인\n",
        "# 마지막 time step의 output을 return\n",
        "rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 64)                6208      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 6,468\n",
            "Trainable params: 6,468\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g3De1ls6LoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "32e33252-0d8c-41aa-9620-297da5e19829"
      },
      "source": [
        "# RNN layer를 정의할 때, return_sequences=True로 설정해주면 시간축이 유지되도록 return\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True), #return sequences 설정해야 모든 timestamp 의 축을 출력해준다\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "rnn(input_sequence)\n",
        "# 시간 축에 따라, 모든 time step에 대한 output을 출력\n",
        "# 마지막 dense layer도 모든 time step에 대해 동일하게 적용됨\n",
        "rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (None, 16, 64)            6208      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16, 4)             260       \n",
            "=================================================================\n",
            "Total params: 6,468\n",
            "Trainable params: 6,468\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTRg8zBQF4Yx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "b0e56a1c-e110-4bcf-ee3c-fbd737456496"
      },
      "source": [
        "# RNN layer를 여러 층 쌓을 때 return_sequences=True가 꼭 필요함\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.SimpleRNN(units),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "rnn(input_sequence)\n",
        "rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 16, 64)            6208      \n",
            "_________________________________________________________________\n",
            "simple_rnn_3 (SimpleRNN)     (None, 16, 64)            8256      \n",
            "_________________________________________________________________\n",
            "simple_rnn_4 (SimpleRNN)     (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 22,980\n",
            "Trainable params: 22,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYKOhpnx7MCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "f795354f-166f-40b4-a6c9-8c2f05f6e93d"
      },
      "source": [
        "# LSTM을 사용하는 방법도 동일함 \n",
        "lstm = tf.keras.Sequential([  \n",
        "  tf.keras.layers.LSTM(units),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "lstm(input_sequence)\n",
        "# 기본 RNN layer보다 훨씬 많은 파라미터 수를 가지고 있음\n",
        "lstm.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 25,092\n",
            "Trainable params: 25,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC80tzdo7kEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6ffc6cce-ab5c-4407-a1b1-d28d19d46a4d"
      },
      "source": [
        "# return_state=True 로 설정하면, LSTM의 마지막 cell state와 hidden state를 모두 return 받을 수 있음\n",
        "# tf.keras.Sequenctial에서는 각 layer의 output이 1개일때만 사용할수 있으므로, 적합하지 않음\n",
        "class LSTM(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):        \n",
        "        whole_seq_output, final_hidden_state, final_cell_state = self.lstm_layer(inputs)\n",
        "        \n",
        "        # shape를 출력\n",
        "        print(\"whole_seq_output :\", whole_seq_output.shape)\n",
        "        print(\"final_hidden_state :\", final_hidden_state.shape)\n",
        "        print(\"final_cell_state :\", final_cell_state.shape)\n",
        "\n",
        "        output_prob = self.dense(whole_seq_output)\n",
        "        return output_prob\n",
        "\n",
        "lstm = LSTM()\n",
        "lstm(input_sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whole_seq_output : (None, 16, 64)\n",
            "final_hidden_state : (None, 64)\n",
            "final_cell_state : (None, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'lstm_1/Identity:0' shape=(None, 16, 4) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKYC7UzpBDaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "43ede209-e190-4513-9680-a6b1ef075633"
      },
      "source": [
        "# GRU는 state가 하나로 되어있음\n",
        "class GRU(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(GRU, self).__init__()\n",
        "        self.gru_layer = tf.keras.layers.GRU(units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):        \n",
        "        whole_seq_output, final_state = self.gru_layer(inputs)\n",
        "\n",
        "        # shape를 출력\n",
        "        print(\"whole_seq_output :\", whole_seq_output.shape)\n",
        "        print(\"final_state :\", final_state.shape)\n",
        "        \n",
        "        output_prob = self.dense(whole_seq_output)\n",
        "        return output_prob\n",
        "\n",
        "gru = GRU()\n",
        "gru(input_sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whole_seq_output : (None, 16, 64)\n",
            "final_state : (None, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'gru/Identity:0' shape=(None, 16, 4) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx6UkuneSmBU",
        "colab_type": "text"
      },
      "source": [
        "## Bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVL7Zu0nCXRk",
        "colab_type": "text"
      },
      "source": [
        "- Bidirectional RNN layer를 만들기 위해서, Wrapper 형태로 활용가능한 layer를 제공함\n",
        "  - tf.keras.layers.Bidirectional\n",
        "- Bidirectional로 활용하고자 하는 RNN layer를 감싸주는 형태로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syhqwD6ZCW0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "c8870fef-56a0-40e7-9426-bd36c865e135"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)))\n",
        "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "model(input_sequence)\n",
        "# 양방향의 hidden state를 모두 출력하므로, output 차원이 2배 (64 * 2)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_2 (Bidirection (None, 128)               49664     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 50,180\n",
            "Trainable params: 50,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIctpo6LIKiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "07e1986c-59fc-45d3-cfec-cbc4b834fbae"
      },
      "source": [
        "# bidirectional RNN layer를 여러 층 쌓는 것도 가능\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)))\n",
        "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "model(input_sequence)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 16, 128)           49664     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 148,996\n",
            "Trainable params: 148,996\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGNOzySqI-os",
        "colab_type": "text"
      },
      "source": [
        "## TimeDistributed 이해 및 활용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu_95Z8fJO8i",
        "colab_type": "text"
      },
      "source": [
        "- 시간 축에 대해 동일한 파라미터의 layer를 적용하는 방법\n",
        "- RNN layer와 직접적으로 관련된 것은 아니지만, sequence 데이터를 다루다보면 꼭 필요한 기능\n",
        "- 동일하게 적용할 layer를 wrapper로 감싸서 사용\n",
        "  - tf.keras.layers.TimeDistributed\n",
        "- Dense layer는 마지막 축(axis)에 대해 계산하므로, TimeDistributed가 자동으로 적용됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2lBvyfKJ6gN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "d12158a8-9fac-4e53-c7fc-140ea3d5ed6e"
      },
      "source": [
        "# RNN layer의 output을 dense layer에 통과 시키는 것은 시간 축에 따라 동일한 파라미터를 활용하는 것임\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "rnn(input_sequence)\n",
        "rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_5 (SimpleRNN)     (None, 16, 64)            6208      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 16, 4)             260       \n",
            "=================================================================\n",
            "Total params: 6,468\n",
            "Trainable params: 6,468\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBlveRc4KGQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "146353bb-72ed-4fc8-ab30-38031a965617"
      },
      "source": [
        "# TimeDistributed를 활용하는 예시\n",
        "# 위의 셀에서도 자동적으로 시간 축에 대해 동일한 파라미터를 사용하므로, 결과는 완전히 동일함\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "])\n",
        "\n",
        "rnn(input_sequence)\n",
        "rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_6 (SimpleRNN)     (None, 16, 64)            6208      \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 16, 4)             260       \n",
            "=================================================================\n",
            "Total params: 6,468\n",
            "Trainable params: 6,468\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gVCRhvwKuCx",
        "colab_type": "text"
      },
      "source": [
        "- 아래는 TimeDistributed가 꼭 필요한 상황의 예시\n",
        "- 비디오 데이터 예시\n",
        "  - sequence length는 10\n",
        "  - 각 frame의 이미지는 128 x 128\n",
        "  - 각 이미지의 채널 수는 3\n",
        "- time step에 따라, 서로 다른 이미지에 동일한 CNN layer를 적용해야 하는 상황"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG79loPTKsbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "d629136a-3f15-49ff-8efe-0d1d25cb7952"
      },
      "source": [
        "# input shape : [sequence length, width, height, channel]\n",
        "inputs = tf.keras.Input(shape=(10, 128, 128, 3))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3)))\n",
        "])\n",
        "\n",
        "model(inputs)\n",
        "# 모든 time step에 대해 동일한 cnn layer 적용된 것 확인\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_1 (TimeDist (None, 10, 126, 126, 64)  1792      \n",
            "=================================================================\n",
            "Total params: 1,792\n",
            "Trainable params: 1,792\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}